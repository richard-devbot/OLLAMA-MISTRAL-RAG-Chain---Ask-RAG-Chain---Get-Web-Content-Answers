import streamlit as st
import bs4
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import WebBaseLoader
from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import OllamaEmbeddings
#import ollama
from langchain.llms import Ollama

#llm=ollama(model="mistral")
llm = Ollama(model="mistral")
# Function to load, split, and retrieve documents
def load_and_retrieve_docs(url):
    loader = WebBaseLoader(
        web_paths=(url,),
        bs_kwargs=dict()
    )
    docs = loader.load()
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
    splits = text_splitter.split_documents(docs)
    embeddings = OllamaEmbeddings(model="mistral")
    vectorstore = Chroma.from_documents(documents=splits, embedding=embeddings)
    return vectorstore.as_retriever()

# Function to format documents
def format_docs(docs):
    return "\n\n".join(doc.page_content for doc in docs)

# Function that defines the RAG chain
def rag_chain(url, question):
    retriever = load_and_retrieve_docs(url)
    retrieved_docs = retriever.invoke(question)
    formatted_context = format_docs(retrieved_docs)
    formatted_prompt = f"Question: {question}\n\nContext: {formatted_context}"
    response = Ollama.chat(model='mistral', messages=[{'role': 'user', 'Content': formatted_prompt}])
    return response['message']['Content']

# Streamlit app
st.sidebar.title(":green[Designed By Richardson Gunde]")
st.sidebar.markdown("""
    🚀 Excited to share a breakthrough in Question Answering! 🔍💡

    Introducing the RAG Chain Question Answering system - your go-to solution for extracting insights from web articles effortlessly. 📰💬

    With this innovative system, powered by cutting-edge technologies like text splitting and embeddings, we're bridging the gap between complex data sources and your queries. 🤝✨

    🔗 Simply input a URL and your question, and watch as the RAG Chain uncovers valuable answers from the provided content. 📝🔎

    Try it out now and experience the future of information retrieval! 💡💼

    #RAGChain #QuestionAnswering #Innovation #AI #InformationRetrieval 
                      
          🚀🚀🚀  Here's a breakdown of how it works: 🚀🚀🚀

    Document Loading: The program loads web documents from a specified URL using a WebBaseLoader.
    Text Splitting: It splits the loaded documents into smaller chunks for better processing using a RecursiveCharacterTextSplitter.
    Embeddings: OllamaEmbeddings are used to embed the text chunks into a vector space, allowing for semantic understanding.
    Vector Store: The embeddings are stored in a vector store, specifically Chroma, to facilitate efficient retrieval.
    Question Answering: When provided with a question, the system retrieves relevant documents from the vector store and formats them. It then constructs a prompt with the question and context and utilizes Ollama's chat function to generate an answer based on the prompt.
    Gradio Interface: The program provides a user-friendly interface using Gradio, where users can input a URL and a question, and receive answers generated by the RAG Chain.
    Overall, the RAG Chain system offers a seamless way to extract insights and answers from web articles, streamlining the process of information retrieval.             
                           
    -->If you want this tool, please let us know.
                        We are open source developers!
                        **Contributions Welcome**
                         
            Linkedin : Richardson Gunde
            Gmail : gunderichardson@gmail.com
    """)
st.title(":green[OLLAMA MISTRAL RAG Chain - Ask RAG Chain--> Get Web Content Answers]")

st.markdown("Ask any question about a web article and RAG Chain will find answers for you!")

url = st.text_input("Enter URL:")
question = st.text_input("Enter question:")

if url and question:
    st.write(rag_chain(url, question))